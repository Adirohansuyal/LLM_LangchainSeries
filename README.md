# ü§ñ Private LLM-Powered Document QA System

A high-performance, retrieval-augmented LLM application built using **LangChain**, **FAISS**, and **Google's Gemma-2B** model via Hugging Face Inference API.

This system enables intelligent, context-aware question answering over **pre-indexed documents** (PDFs) ‚Äî without exposing document uploads to end users. Designed for private, secure, and performant document intelligence applications.

---

## üí° Key Features

| Capability                             | Description                                                                 |
|----------------------------------------|-----------------------------------------------------------------------------|
| üß† **LLM-Driven QA**                    | Uses `google/gemma-2b-it` for response generation                          |
| üîç **RAG Pipeline**                    | Retrieval-Augmented Generation with semantic search over vector DB         |
| üß© **Chunk-Aware Indexing**             | Text is split into optimized chunks for embedding                          |
| üß¨ **Vector Embeddings**                | Encodes text with `MiniLM` (sentence-transformers)                         |
| ‚ö° **FAISS**                            | Fast and scalable vector similarity search                                 |
| üß± **LangChain**                        | Powers document loaders, chunking, embedding & retrieval chain             |
| üõ°Ô∏è **Admin-Controlled Document Access** | Only backend PDFs are queried ‚Äî users cannot upload or modify documents    |
| üñ•Ô∏è **MacBook Air Optimized**            | Entirely API-based ‚Äî no GPU needed, light CPU usage                        |

---

## üß† System Architecture

```mermaid
flowchart TD
    A[PDF Documents] --> B[LangChain Chunking]
    B --> C[MiniLM Embeddings]
    C --> D[FAISS Vector Store]

    E[User Question] --> F[FAISS Retriever]
    D --> F
    F --> G[Top-K Context]
    G --> H[Gemma-2B LLM API]
    H --> I[Final Answer]
```

---

## ‚öôÔ∏è Technology Stack

- `LangChain`: modular pipeline for document Q&A
- `FAISS`: efficient vector search
- `sentence-transformers/all-MiniLM-L6-v2`: compact, performant embedding model
- `Hugging Face InferenceClient`: connects to `google/gemma-2b-it`
- `Streamlit`: UI for question submission and response display
- `pickle`: persistent vector index caching

---

## üß™ How It Works

1. **Admin loads PDF(s)** (e.g. a resume or private document)
2. LangChain splits text into **semantic chunks**
3. Each chunk is embedded via **MiniLM**
4. FAISS indexes the embeddings
5. At runtime:
   - A user question is vector-matched with the closest text chunks
   - The top-k chunks are passed as context to **Gemma-2B**
   - The LLM generates a natural language answer based on private context

---

## üîê Use Cases

- Resume or profile Q&A assistant
- Secure document intelligence systems (e.g. policy, contract, research docs)
- Internal knowledge base automation
- Educational Q&A interfaces

---

## üõ†Ô∏è How to Use

### 1. Clone the Repository
```bash
git clone https://github.com/your-org/private-llm-docqa.git
cd private-llm-docqa
```

### 2. Install Dependencies
```bash
pip install -r requirements.txt
```

### 3. Place Your PDFs
Put your private documents in the `docs/` folder, e.g.:
```
docs/Aditya_Suyal_Resume.pdf
```

### 4. Run Vectorization Script (Admin Only)
```bash
python admin_vectorize.py
```

This will generate `vectorstore_index.pkl`.

### 5. Launch Streamlit App
```bash
streamlit run app.py
```

---

## üìÇ Project Structure

```
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îî‚îÄ‚îÄ Aditya_Suyal_Resume.pdf
‚îú‚îÄ‚îÄ admin_vectorize.py      # Embedding + vector index generator
‚îú‚îÄ‚îÄ app.py                  # Frontend Streamlit Q&A app
‚îú‚îÄ‚îÄ vectorstore_index.pkl   # Precomputed semantic index
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ requirements.txt
```

---

## ü§ñ Example Questions You Can Ask

- "What programming languages does Aditya know?"
- "List the academic qualifications."
- "What achievements are mentioned in the resume?"
- "Summarize the document."

---

## üß† Why It‚Äôs an LLM Project

- ‚úÖ Powered by a **state-of-the-art instruction-tuned LLM**
- ‚úÖ Implements the **Retrieval-Augmented Generation (RAG)** pattern
- ‚úÖ Combines **dense vector similarity search** with **semantic LLM reasoning**
- ‚úÖ Uses **LangChain**, the standard framework for LLM orchestration
- ‚úÖ Delivers intelligent responses based on **private, structured document knowledge**

---

## üìÑ License

MIT License ‚Äì open for research, commercial adaptation, and educational use.

---

> Built with ‚ù§Ô∏è using open LLMs, FAISS, and LangChain. Optimized for lightweight deployment on everyday machines.